{"changed":true,"filter":false,"title":"procesar.py","tooltip":"/procesar/procesar.py","value":"import boto3\nimport csv\nimport datetime\nfrom bs4 import BeautifulSoup\n\ns3_client = boto3.client(\"s3\")\nBUCKET_DESTINO = \"recibiendo\"  # cambia si usas otro bucket\n\ndef app(event, context):\n    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")\n\n    # Descargar archivo HTML desde S3\n    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)\n    contenido_html = response[\"Body\"].read().decode(\"utf-8\")\n    soup = BeautifulSoup(contenido_html, \"html.parser\")\n\n    # Detectar nombre del periódico por el nombre del archivo\n    if \"espectador\" in archivo_html.lower():\n        periodico = \"elespectador\"\n    else:\n        periodico = \"eltiempo\"\n\n    noticias = []\n\n    # Extraer noticias según el periódico\n    if periodico == \"eltiempo\":\n        for noticia in soup.select(\"article\"):\n            categoria = noticia.find(\"a\", class_=\"category\")\n            titulo = noticia.find(\"h3\")\n            link = noticia.find(\"a\", href=True)\n\n            if categoria and titulo and link:\n                noticias.append({\n                    \"Categoria\": categoria.text.strip(),\n                    \"Titular\": titulo.text.strip(),\n                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"]\n                })\n    elif periodico == \"elespectador\":\n        for noticia in soup.select(\"section article\"):\n            categoria = noticia.find(\"span\", class_=\"section\")\n            titulo = noticia.find(\"h2\") or noticia.find(\"h3\")\n            link = noticia.find(\"a\", href=True)\n\n            if categoria and titulo and link:\n                noticias.append({\n                    \"Categoria\": categoria.text.strip(),\n                    \"Titular\": titulo.text.strip(),\n                    \"Link\": link[\"href\"] if link[\"href\"].startswith(\"http\") else \"https://www.elespectador.com\" + link[\"href\"]\n                })\n\n    # Fecha para la ruta\n    hoy = datetime.datetime.now()\n    year = hoy.strftime(\"%Y\")\n    month = hoy.strftime(\"%m\")\n    day = hoy.strftime(\"%d\")\n\n    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]\n    ruta_local = f\"/tmp/{nombre_csv}\"\n    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"\n\n    # Crear CSV\n    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n        campos = ['Categoria', 'Titular', 'Link']\n        writer = csv.DictWriter(csvfile, fieldnames=campos)\n        writer.writeheader()\n        writer.writerows(noticias)\n\n    # Subir CSV a S3\n    s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)\n    print(f\"CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")\n\n    return {}\n","undoManager":{"mark":-2,"position":1,"stack":[[{"start":{"row":0,"column":0},"end":{"row":74,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"eltiempop\"  # cambia si usas otro bucket","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    # Descargar archivo HTML desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    # Detectar nombre del periódico por el nombre del archivo","    if \"espectador\" in archivo_html.lower():","        periodico = \"elespectador\"","    else:","        periodico = \"eltiempo\"","","    noticias = []","","    # Extraer noticias según el periódico","    if periodico == \"eltiempo\":","        for noticia in soup.select(\"article\"):","            categoria = noticia.find(\"a\", class_=\"category\")","            titulo = noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"]","                })","    elif periodico == \"elespectador\":","        for noticia in soup.select(\"section article\"):","            categoria = noticia.find(\"span\", class_=\"section\")","            titulo = noticia.find(\"h2\") or noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": link[\"href\"] if link[\"href\"].startswith(\"http\") else \"https://www.elespectador.com\" + link[\"href\"]","                })","","    # Fecha para la ruta","    hoy = datetime.datetime.now()","    year = hoy.strftime(\"%Y\")","    month = hoy.strftime(\"%m\")","    day = hoy.strftime(\"%d\")","","    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]","    ruta_local = f\"/tmp/{nombre_csv}\"","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    # Crear CSV","    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos)","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir CSV a S3","    s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)","    print(f\"CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")","","    return {}",""],"id":1}],[{"start":{"row":6,"column":18},"end":{"row":6,"column":27},"action":"remove","lines":["eltiempop"],"id":2},{"start":{"row":6,"column":18},"end":{"row":6,"column":19},"action":"insert","lines":["r"]},{"start":{"row":6,"column":19},"end":{"row":6,"column":20},"action":"insert","lines":["e"]},{"start":{"row":6,"column":20},"end":{"row":6,"column":21},"action":"insert","lines":["c"]},{"start":{"row":6,"column":21},"end":{"row":6,"column":22},"action":"insert","lines":["i"]},{"start":{"row":6,"column":22},"end":{"row":6,"column":23},"action":"insert","lines":["b"]},{"start":{"row":6,"column":23},"end":{"row":6,"column":24},"action":"insert","lines":["i"]},{"start":{"row":6,"column":24},"end":{"row":6,"column":25},"action":"insert","lines":["e"]},{"start":{"row":6,"column":25},"end":{"row":6,"column":26},"action":"insert","lines":["n"]},{"start":{"row":6,"column":26},"end":{"row":6,"column":27},"action":"insert","lines":["d"]},{"start":{"row":6,"column":27},"end":{"row":6,"column":28},"action":"insert","lines":["o"]}]]},"ace":{"folds":[],"scrolltop":932,"scrollleft":0,"selection":{"start":{"row":6,"column":28},"end":{"row":6,"column":28},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":65,"state":"start","mode":"ace/mode/python"}},"timestamp":1748481216957}