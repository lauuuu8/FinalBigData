{"changed":true,"filter":false,"title":"procesar.py","tooltip":"/procesar/procesar.py","value":"import boto3\nimport os\nimport csv\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport urllib.parse\nimport json\nimport io\n\ns3 = boto3.client('s3')\n\ndef app(event, context): \n    print(\"==== Evento recibido por Lambda ====\")\n    print(json.dumps(event, indent=2))\n    \n    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'recibiendo')\n    BASE_URLS = {\n        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),\n        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')\n    }\n    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()\n\n    if 'Records' not in event:\n        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"\n        print(error_msg)\n        return {\n            \"statusCode\": 400,\n            \"body\": json.dumps({\"error\": error_msg})\n        }\n\n    for record in event['Records']:\n        try:\n            bucket = record['s3']['bucket']['name']\n            key = urllib.parse.unquote_plus(record['s3']['object']['key'])\n\n            if not key.startswith('headlines/raw/') or not key.endswith('.html'):\n                print(f\"Ignorando archivo no válido para procesamiento: {key}\")\n                continue\n\n            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")\n\n            response = s3.get_object(Bucket=bucket, Key=key)\n            content = response['Body'].read().decode('utf-8')\n            print(f\"Archivo HTML extraído correctamente: {key}\")\n\n            soup = BeautifulSoup(content, 'html.parser')\n            nombre_archivo = os.path.basename(key)\n\n            if 'publimetro' in nombre_archivo:\n                periodico = 'publimetro'\n            elif 'eltiempo' in nombre_archivo:\n                periodico = 'eltiempo'\n            else:\n                periodico = 'desconocido'\n                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")\n\n            noticias = []\n\n            for article in soup.find_all('article'):\n                titular_tag = article.find(['h1', 'h2', 'h3'])\n                enlace_tag = article.find('a', href=True)\n\n                if titular_tag and enlace_tag:\n                    titular = titular_tag.get_text(strip=True)\n                    enlace = enlace_tag['href']\n                    \n                    if not enlace.startswith('http'):\n                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"\n\n                    categoria = ''\n                    parts = enlace.split('/')\n                    if len(parts) > 3:\n                        categoria = parts[3]\n\n                    noticias.append({\n                        'categoria': categoria,\n                        'titular': titular,\n                        'enlace': enlace\n                    })\n\n            if not noticias:\n                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")\n                for heading in soup.find_all(['h1', 'h2', 'h3']):\n                    a_tag = heading.find('a', href=True)\n                    if a_tag:\n                        titular = heading.get_text(strip=True)\n                        enlace = a_tag['href']\n                        \n                        if not enlace.startswith('http'):\n                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"\n\n                        categoria = ''\n                        parts = enlace.split('/')\n                        if len(parts) > 3:\n                            categoria = parts[3]\n\n                        noticias.append({\n                            'categoria': categoria,\n                            'titular': titular,\n                            'enlace': enlace\n                        })\n\n            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")\n\n            fecha = datetime.utcnow()\n            year = fecha.strftime('%Y')\n            month = fecha.strftime('%m')\n            day = fecha.strftime('%d')\n            hour = fecha.strftime('%H')\n            minute = fecha.strftime('%M')\n\n            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"\n            print(f\"Guardando CSV en: {csv_key}\")\n\n            csv_buffer = io.StringIO()\n            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])\n            writer.writeheader()\n            for noticia in noticias:\n                writer.writerow(noticia)\n\n            s3.put_object(\n                Bucket=bucket,\n                Key=csv_key,\n                Body=csv_buffer.getvalue(),\n                ContentType='text/csv'\n            )\n            print(\"Archivo CSV subido exitosamente a S3.\")\n\n        except Exception as e:\n            print(f\"Error procesando el archivo {key}: {str(e)}\")\n            continue\n\n    return {\n        \"statusCode\": 200,\n        \"body\": json.dumps({\n            \"message\": \"Procesamiento completado\",\n            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)\n        })\n    }\n","undoManager":{"mark":71,"position":72,"stack":[[{"start":{"row":0,"column":0},"end":{"row":74,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"eltiempop\"  # cambia si usas otro bucket","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    # Descargar archivo HTML desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    # Detectar nombre del periódico por el nombre del archivo","    if \"espectador\" in archivo_html.lower():","        periodico = \"elespectador\"","    else:","        periodico = \"eltiempo\"","","    noticias = []","","    # Extraer noticias según el periódico","    if periodico == \"eltiempo\":","        for noticia in soup.select(\"article\"):","            categoria = noticia.find(\"a\", class_=\"category\")","            titulo = noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"]","                })","    elif periodico == \"elespectador\":","        for noticia in soup.select(\"section article\"):","            categoria = noticia.find(\"span\", class_=\"section\")","            titulo = noticia.find(\"h2\") or noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": link[\"href\"] if link[\"href\"].startswith(\"http\") else \"https://www.elespectador.com\" + link[\"href\"]","                })","","    # Fecha para la ruta","    hoy = datetime.datetime.now()","    year = hoy.strftime(\"%Y\")","    month = hoy.strftime(\"%m\")","    day = hoy.strftime(\"%d\")","","    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]","    ruta_local = f\"/tmp/{nombre_csv}\"","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    # Crear CSV","    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos)","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir CSV a S3","    s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)","    print(f\"CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")","","    return {}",""],"id":1}],[{"start":{"row":6,"column":18},"end":{"row":6,"column":27},"action":"remove","lines":["eltiempop"],"id":2},{"start":{"row":6,"column":18},"end":{"row":6,"column":19},"action":"insert","lines":["r"]},{"start":{"row":6,"column":19},"end":{"row":6,"column":20},"action":"insert","lines":["e"]},{"start":{"row":6,"column":20},"end":{"row":6,"column":21},"action":"insert","lines":["c"]},{"start":{"row":6,"column":21},"end":{"row":6,"column":22},"action":"insert","lines":["i"]},{"start":{"row":6,"column":22},"end":{"row":6,"column":23},"action":"insert","lines":["b"]},{"start":{"row":6,"column":23},"end":{"row":6,"column":24},"action":"insert","lines":["i"]},{"start":{"row":6,"column":24},"end":{"row":6,"column":25},"action":"insert","lines":["e"]},{"start":{"row":6,"column":25},"end":{"row":6,"column":26},"action":"insert","lines":["n"]},{"start":{"row":6,"column":26},"end":{"row":6,"column":27},"action":"insert","lines":["d"]},{"start":{"row":6,"column":27},"end":{"row":6,"column":28},"action":"insert","lines":["o"]}],[{"start":{"row":0,"column":0},"end":{"row":74,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"recibiendo\"  # cambia si usas otro bucket","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    # Descargar archivo HTML desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    # Detectar nombre del periódico por el nombre del archivo","    if \"espectador\" in archivo_html.lower():","        periodico = \"elespectador\"","    else:","        periodico = \"eltiempo\"","","    noticias = []","","    # Extraer noticias según el periódico","    if periodico == \"eltiempo\":","        for noticia in soup.select(\"article\"):","            categoria = noticia.find(\"a\", class_=\"category\")","            titulo = noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"]","                })","    elif periodico == \"elespectador\":","        for noticia in soup.select(\"section article\"):","            categoria = noticia.find(\"span\", class_=\"section\")","            titulo = noticia.find(\"h2\") or noticia.find(\"h3\")","            link = noticia.find(\"a\", href=True)","","            if categoria and titulo and link:","                noticias.append({","                    \"Categoria\": categoria.text.strip(),","                    \"Titular\": titulo.text.strip(),","                    \"Link\": link[\"href\"] if link[\"href\"].startswith(\"http\") else \"https://www.elespectador.com\" + link[\"href\"]","                })","","    # Fecha para la ruta","    hoy = datetime.datetime.now()","    year = hoy.strftime(\"%Y\")","    month = hoy.strftime(\"%m\")","    day = hoy.strftime(\"%d\")","","    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]","    ruta_local = f\"/tmp/{nombre_csv}\"","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    # Crear CSV","    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos)","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir CSV a S3","    s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)","    print(f\"CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")","","    return {}",""],"id":3},{"start":{"row":0,"column":0},"end":{"row":97,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import json","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"eltiempop\"  # <-- Ajustá el nombre de tu bucket","","def extraer_noticias_eltiempo(html):","    soup = BeautifulSoup(html, \"html.parser\")","    noticias = []","","    for script in soup.find_all(\"script\", type=\"application/ld+json\"):","        if \"ReportageNewsArticle\" in script.text:","            try:","                data = json.loads(script.text)","                if isinstance(data, list):","                    for item in data:","                        if item.get(\"@type\") == \"ReportageNewsArticle\":","                            noticias.append({","                                \"Categoria\": item.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").split(\"/\")[3] or \"Sin categoría\",","                                \"Titular\": item.get(\"headline\", \"\").strip(),","                                \"Link\": item.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").strip()","                            })","                elif isinstance(data, dict):","                    if data.get(\"@type\") == \"ReportageNewsArticle\":","                        noticias.append({","                            \"Categoria\": data.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").split(\"/\")[3] or \"Sin categoría\",","                            \"Titular\": data.get(\"headline\", \"\").strip(),","                            \"Link\": data.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").strip()","                        })","            except Exception as e:","                print(f\"Error JSON-LD El Tiempo: {e}\")","    return noticias","","def extraer_noticias_publimetro(html):","    soup = BeautifulSoup(html, \"html.parser\")","    noticias = []","","    for article in soup.find_all(\"article\"):","        categoria_tag = article.find(\"span\", class_=\"c-overline\")","        titulo_tag = article.find(\"h2\") or article.find(\"h3\")","        link_tag = article.find(\"a\", href=True)","","        if categoria_tag and titulo_tag and link_tag:","            noticias.append({","                \"Categoria\": categoria_tag.text.strip(),","                \"Titular\": titulo_tag.text.strip(),","                \"Link\": \"https://www.publimetro.co\" + link_tag[\"href\"]","            })","    return noticias","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"📄 Procesando: {archivo_html}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    # Detectar el medio según el nombre del archivo","    if \"eltiempo\" in archivo_html.lower():","        periodico = \"eltiempo\"","        noticias = extraer_noticias_eltiempo(contenido_html)","    elif \"publimetro\" in archivo_html.lower():","        periodico = \"publimetro\"","        noticias = extraer_noticias_publimetro(contenido_html)","    else:","        print(\"❌ No se reconoce el medio.\")","        return {}","","    print(f\"✅ Se extrajeron {len(noticias)} noticias de {periodico}\")","","    hoy = datetime.datetime.now()","    year = hoy.strftime(\"%Y\")","    month = hoy.strftime(\"%m\")","    day = hoy.strftime(\"%d\")","","    # Nombre del archivo CSV","    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]","    ruta_local = f\"/tmp/{nombre_csv}\"","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos)","        writer.writeheader()","        writer.writerows(noticias)","","    try:","        s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)","        print(f\"🚀 CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")","    except Exception as e:","        print(f\"❌ Error al subir CSV a S3: {e}\")","","    return {}",""]}],[{"start":{"row":92,"column":18},"end":{"row":92,"column":19},"action":"remove","lines":[" "],"id":4},{"start":{"row":92,"column":16},"end":{"row":92,"column":18},"action":"remove","lines":["🚀"]}],[{"start":{"row":94,"column":18},"end":{"row":94,"column":19},"action":"remove","lines":["E"],"id":5},{"start":{"row":94,"column":17},"end":{"row":94,"column":18},"action":"remove","lines":[" "]}],[{"start":{"row":94,"column":17},"end":{"row":94,"column":18},"action":"remove","lines":["r"],"id":6},{"start":{"row":94,"column":16},"end":{"row":94,"column":17},"action":"remove","lines":["❌"]}],[{"start":{"row":94,"column":16},"end":{"row":94,"column":17},"action":"insert","lines":["E"],"id":7},{"start":{"row":94,"column":17},"end":{"row":94,"column":18},"action":"insert","lines":["r"]}],[{"start":{"row":72,"column":14},"end":{"row":72,"column":15},"action":"remove","lines":["S"],"id":8},{"start":{"row":72,"column":13},"end":{"row":72,"column":14},"action":"remove","lines":[" "]},{"start":{"row":72,"column":12},"end":{"row":72,"column":13},"action":"remove","lines":["✅"]}],[{"start":{"row":56,"column":12},"end":{"row":56,"column":15},"action":"remove","lines":["📄 "],"id":9}],[{"start":{"row":7,"column":29},"end":{"row":7,"column":65},"action":"remove","lines":[" # <-- Ajustá el nombre de tu bucket"],"id":10}],[{"start":{"row":7,"column":18},"end":{"row":7,"column":27},"action":"remove","lines":["eltiempop"],"id":11},{"start":{"row":7,"column":18},"end":{"row":7,"column":19},"action":"insert","lines":["r"]},{"start":{"row":7,"column":19},"end":{"row":7,"column":20},"action":"insert","lines":["e"]},{"start":{"row":7,"column":20},"end":{"row":7,"column":21},"action":"insert","lines":["c"]},{"start":{"row":7,"column":21},"end":{"row":7,"column":22},"action":"insert","lines":["i"]},{"start":{"row":7,"column":22},"end":{"row":7,"column":23},"action":"insert","lines":["b"]},{"start":{"row":7,"column":23},"end":{"row":7,"column":24},"action":"insert","lines":["i"]},{"start":{"row":7,"column":24},"end":{"row":7,"column":25},"action":"insert","lines":["e"]},{"start":{"row":7,"column":25},"end":{"row":7,"column":26},"action":"insert","lines":["n"]},{"start":{"row":7,"column":26},"end":{"row":7,"column":27},"action":"insert","lines":["d"]},{"start":{"row":7,"column":27},"end":{"row":7,"column":28},"action":"insert","lines":["o"]}],[{"start":{"row":69,"column":17},"end":{"row":69,"column":18},"action":"remove","lines":["N"],"id":12},{"start":{"row":69,"column":16},"end":{"row":69,"column":17},"action":"remove","lines":[" "]},{"start":{"row":69,"column":15},"end":{"row":69,"column":16},"action":"remove","lines":["❌"]}],[{"start":{"row":69,"column":15},"end":{"row":69,"column":16},"action":"insert","lines":["N"],"id":13}],[{"start":{"row":0,"column":0},"end":{"row":97,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import json","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"recibiendo\" ","","def extraer_noticias_eltiempo(html):","    soup = BeautifulSoup(html, \"html.parser\")","    noticias = []","","    for script in soup.find_all(\"script\", type=\"application/ld+json\"):","        if \"ReportageNewsArticle\" in script.text:","            try:","                data = json.loads(script.text)","                if isinstance(data, list):","                    for item in data:","                        if item.get(\"@type\") == \"ReportageNewsArticle\":","                            noticias.append({","                                \"Categoria\": item.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").split(\"/\")[3] or \"Sin categoría\",","                                \"Titular\": item.get(\"headline\", \"\").strip(),","                                \"Link\": item.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").strip()","                            })","                elif isinstance(data, dict):","                    if data.get(\"@type\") == \"ReportageNewsArticle\":","                        noticias.append({","                            \"Categoria\": data.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").split(\"/\")[3] or \"Sin categoría\",","                            \"Titular\": data.get(\"headline\", \"\").strip(),","                            \"Link\": data.get(\"mainEntityOfPage\", {}).get(\"@id\", \"\").strip()","                        })","            except Exception as e:","                print(f\"Error JSON-LD El Tiempo: {e}\")","    return noticias","","def extraer_noticias_publimetro(html):","    soup = BeautifulSoup(html, \"html.parser\")","    noticias = []","","    for article in soup.find_all(\"article\"):","        categoria_tag = article.find(\"span\", class_=\"c-overline\")","        titulo_tag = article.find(\"h2\") or article.find(\"h3\")","        link_tag = article.find(\"a\", href=True)","","        if categoria_tag and titulo_tag and link_tag:","            noticias.append({","                \"Categoria\": categoria_tag.text.strip(),","                \"Titular\": titulo_tag.text.strip(),","                \"Link\": \"https://www.publimetro.co\" + link_tag[\"href\"]","            })","    return noticias","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando: {archivo_html}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    # Detectar el medio según el nombre del archivo","    if \"eltiempo\" in archivo_html.lower():","        periodico = \"eltiempo\"","        noticias = extraer_noticias_eltiempo(contenido_html)","    elif \"publimetro\" in archivo_html.lower():","        periodico = \"publimetro\"","        noticias = extraer_noticias_publimetro(contenido_html)","    else:","        print(\"No se reconoce el medio.\")","        return {}","","    print(f\"e extrajeron {len(noticias)} noticias de {periodico}\")","","    hoy = datetime.datetime.now()","    year = hoy.strftime(\"%Y\")","    month = hoy.strftime(\"%m\")","    day = hoy.strftime(\"%d\")","","    # Nombre del archivo CSV","    nombre_csv = archivo_html.replace(\".html\", \".csv\").split(\"/\")[-1]","    ruta_local = f\"/tmp/{nombre_csv}\"","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    with open(ruta_local, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos)","        writer.writeheader()","        writer.writerows(noticias)","","    try:","        s3_client.upload_file(ruta_local, BUCKET_DESTINO, ruta_s3)","        print(f\"CSV subido a s3://{BUCKET_DESTINO}/{ruta_s3}\")","    except Exception as e:","        print(f\"Error al subir CSV a S3: {e}\")","","    return {}",""],"id":19},{"start":{"row":0,"column":0},"end":{"row":198,"column":5},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os # Para manipulación de rutas y nombres de archivo","","# Configura el cliente S3","s3_client = boto3.client(\"s3\")","","# REEMPLAZA ESTE VALOR con el nombre de tu bucket S3 de destino para los archivos CSV.","BUCKET_DESTINO_TRUSTED = \"TU_BUCKET_DE_DESTINO_AQUI\"","","# URLs base para construir enlaces absolutos si es necesario","BASE_URLS = {","    \"eltiempo\": \"https://www.eltiempo.com\",","    \"publimetro\": \"https://www.publimetro.co\" # Verifica si es .co o .com u otro TLD para Colombia","}","","def extract_data_eltiempo(soup, base_url):","    \"\"\"","    Extrae noticias del HTML de El Tiempo.","    \"\"\"","    noticias = []","    # Selector para los contenedores de artículos basado en 'eltiempo-contenido-2025-05-29.html'","    article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","","    for item in article_items:","        categoria_texto = None","        titular_texto = None","        enlace_url = None","","        try:","            # Categoría","            # La categoría está dentro de un <div class=\"category\">","            category_tag = item.find(\"div\", class_=\"category\")","            if category_tag:","                categoria_texto = category_tag.get_text(strip=True)","","            # Titular y Enlace","            # El titular y enlace están en <h3 class=\"title-container initial-font medium\"> -> <a class=\"title page-link\">","            title_container = item.find(\"h3\", class_=\"title-container\")","            if title_container:","                title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                if title_link_tag:","                    titular_texto = title_link_tag.get_text(strip=True)","                    href = title_link_tag.get('href')","                    if href:","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url + href","            ","            # Solo agregar si tenemos titular y enlace, la categoría puede ser opcional","            if titular_texto and enlace_url:","                noticias.append({","                    \"categoria\": categoria_texto if categoria_texto else \"N/A\", # Usar N/A si no hay categoría","                    \"titular\": titular_texto,","                    \"enlace\": enlace_url","                })","        except Exception as e:","            print(f\"Error parseando un item de El Tiempo: {e}\")","            # Considera loggear el item.prettify() para debug si es necesario","            continue","            ","    return noticias","","def extract_data_publimetro(soup, base_url):","    \"\"\"","    Extrae noticias del HTML de Publimetro.","    \"\"\"","    noticias = []","    # Selector para los contenedores de artículos basado en 'publimetro-contenido-2025-05-29.html'","    article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","    ","    for item in article_items:","        categoria_texto = None","        titular_texto = None","        enlace_url = None","        ","        try:","            # Titular y Enlace","            # El titular y enlace están en <a class=\"sm-promo-headline-link\">","            title_anchor = item.find(\"a\", class_=\"sm-promo-headline-link\")","            if title_anchor:","                titular_texto = title_anchor.get_text(strip=True)","                href = title_anchor.get('href')","                if href:","                    if href.startswith(\"http\"):","                        enlace_url = href","                    else:","                        enlace_url = base_url + href","            ","            # Categoría","            # La categoría está en <a class=\"sm-promo-label-primary-link\"> -> <span>","            category_anchor = item.find(\"a\", class_=\"sm-promo-label-primary-link\")","            if category_anchor:","                category_span = category_anchor.find(\"span\")","                if category_span:","                    categoria_texto = category_span.get_text(strip=True)","                else: # Fallback si el texto está directamente en <a> y no en <span>","                    categoria_texto = category_anchor.get_text(strip=True)","","            # Solo agregar si tenemos titular y enlace","            if titular_texto and enlace_url:","                noticias.append({","                    \"categoria\": categoria_texto if categoria_texto else \"N/A\", # Usar N/A si no hay categoría","                    \"titular\": titular_texto,","                    \"enlace\": enlace_url","                })","        except Exception as e:","            print(f\"Error parseando un item de Publimetro: {e}\")","            # Considera loggear el item.prettify() para debug si es necesario","            continue","            ","    return noticias","","def app(event, context):","    \"\"\"","    Función principal de AWS Lambda.","    \"\"\"","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"] # ej: 'raw/eltiempo-2025-05-29.html'","    ","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","    ","    # Obtener el contenido del archivo HTML desde S3","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","        ","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    datos_noticias = []","    periodico_identificado = None","    ","    nombre_archivo_original = os.path.basename(archivo_html_key).lower()","","    if \"eltiempo\" in nombre_archivo_original:","        periodico_identificado = \"eltiempo\"","        base_url = BASE_URLS[periodico_identificado]","        datos_noticias = extract_data_eltiempo(soup, base_url)","    elif \"publimetro\" in nombre_archivo_original:","        periodico_identificado = \"publimetro\"","        base_url = BASE_URLS[periodico_identificado]","        datos_noticias = extract_data_publimetro(soup, base_url)","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. No se puede determinar la fuente (eltiempo/publimetro).\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido\"}","","    if not datos_noticias:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","        ","    # Preparar nombre y ruta para el archivo CSV","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","    ","    # Nombre del archivo CSV basado en el nombre del HTML original","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","    ","    # Ruta temporal en Lambda para guardar el CSV antes de subirlo","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\" ","    ","    # Escribir los datos al archivo CSV","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';') # Usando ; como delimitador","            ","            writer.writeheader()","            writer.writerows(datos_noticias)","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","        ","    # Ruta de destino en S3 para el archivo CSV","    # Formato: TRUSTED-LAYER/periodico={periodico}/year={YYYY}/month={MM}/day={DD}/{nombre_original}.csv","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","    ","    # Subir el archivo CSV al bucket de destino","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO_TRUSTED, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","        ","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv\": f\"s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\"","    }"]}],[{"start":{"row":10,"column":26},"end":{"row":10,"column":51},"action":"remove","lines":["TU_BUCKET_DE_DESTINO_AQUI"],"id":20},{"start":{"row":10,"column":26},"end":{"row":10,"column":27},"action":"insert","lines":["r"]},{"start":{"row":10,"column":27},"end":{"row":10,"column":28},"action":"insert","lines":["e"]},{"start":{"row":10,"column":28},"end":{"row":10,"column":29},"action":"insert","lines":["c"]},{"start":{"row":10,"column":29},"end":{"row":10,"column":30},"action":"insert","lines":["i"]},{"start":{"row":10,"column":30},"end":{"row":10,"column":31},"action":"insert","lines":["b"]},{"start":{"row":10,"column":31},"end":{"row":10,"column":32},"action":"insert","lines":["i"]},{"start":{"row":10,"column":32},"end":{"row":10,"column":33},"action":"insert","lines":["e"]},{"start":{"row":10,"column":33},"end":{"row":10,"column":34},"action":"insert","lines":["n"]},{"start":{"row":10,"column":34},"end":{"row":10,"column":35},"action":"insert","lines":["d"]},{"start":{"row":10,"column":35},"end":{"row":10,"column":36},"action":"insert","lines":["o"]}],[{"start":{"row":0,"column":0},"end":{"row":198,"column":5},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os # Para manipulación de rutas y nombres de archivo","","# Configura el cliente S3","s3_client = boto3.client(\"s3\")","","# REEMPLAZA ESTE VALOR con el nombre de tu bucket S3 de destino para los archivos CSV.","BUCKET_DESTINO_TRUSTED = \"recibiendo\"","","# URLs base para construir enlaces absolutos si es necesario","BASE_URLS = {","    \"eltiempo\": \"https://www.eltiempo.com\",","    \"publimetro\": \"https://www.publimetro.co\" # Verifica si es .co o .com u otro TLD para Colombia","}","","def extract_data_eltiempo(soup, base_url):","    \"\"\"","    Extrae noticias del HTML de El Tiempo.","    \"\"\"","    noticias = []","    # Selector para los contenedores de artículos basado en 'eltiempo-contenido-2025-05-29.html'","    article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","","    for item in article_items:","        categoria_texto = None","        titular_texto = None","        enlace_url = None","","        try:","            # Categoría","            # La categoría está dentro de un <div class=\"category\">","            category_tag = item.find(\"div\", class_=\"category\")","            if category_tag:","                categoria_texto = category_tag.get_text(strip=True)","","            # Titular y Enlace","            # El titular y enlace están en <h3 class=\"title-container initial-font medium\"> -> <a class=\"title page-link\">","            title_container = item.find(\"h3\", class_=\"title-container\")","            if title_container:","                title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                if title_link_tag:","                    titular_texto = title_link_tag.get_text(strip=True)","                    href = title_link_tag.get('href')","                    if href:","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url + href","            ","            # Solo agregar si tenemos titular y enlace, la categoría puede ser opcional","            if titular_texto and enlace_url:","                noticias.append({","                    \"categoria\": categoria_texto if categoria_texto else \"N/A\", # Usar N/A si no hay categoría","                    \"titular\": titular_texto,","                    \"enlace\": enlace_url","                })","        except Exception as e:","            print(f\"Error parseando un item de El Tiempo: {e}\")","            # Considera loggear el item.prettify() para debug si es necesario","            continue","            ","    return noticias","","def extract_data_publimetro(soup, base_url):","    \"\"\"","    Extrae noticias del HTML de Publimetro.","    \"\"\"","    noticias = []","    # Selector para los contenedores de artículos basado en 'publimetro-contenido-2025-05-29.html'","    article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","    ","    for item in article_items:","        categoria_texto = None","        titular_texto = None","        enlace_url = None","        ","        try:","            # Titular y Enlace","            # El titular y enlace están en <a class=\"sm-promo-headline-link\">","            title_anchor = item.find(\"a\", class_=\"sm-promo-headline-link\")","            if title_anchor:","                titular_texto = title_anchor.get_text(strip=True)","                href = title_anchor.get('href')","                if href:","                    if href.startswith(\"http\"):","                        enlace_url = href","                    else:","                        enlace_url = base_url + href","            ","            # Categoría","            # La categoría está en <a class=\"sm-promo-label-primary-link\"> -> <span>","            category_anchor = item.find(\"a\", class_=\"sm-promo-label-primary-link\")","            if category_anchor:","                category_span = category_anchor.find(\"span\")","                if category_span:","                    categoria_texto = category_span.get_text(strip=True)","                else: # Fallback si el texto está directamente en <a> y no en <span>","                    categoria_texto = category_anchor.get_text(strip=True)","","            # Solo agregar si tenemos titular y enlace","            if titular_texto and enlace_url:","                noticias.append({","                    \"categoria\": categoria_texto if categoria_texto else \"N/A\", # Usar N/A si no hay categoría","                    \"titular\": titular_texto,","                    \"enlace\": enlace_url","                })","        except Exception as e:","            print(f\"Error parseando un item de Publimetro: {e}\")","            # Considera loggear el item.prettify() para debug si es necesario","            continue","            ","    return noticias","","def app(event, context):","    \"\"\"","    Función principal de AWS Lambda.","    \"\"\"","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"] # ej: 'raw/eltiempo-2025-05-29.html'","    ","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","    ","    # Obtener el contenido del archivo HTML desde S3","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","        ","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    datos_noticias = []","    periodico_identificado = None","    ","    nombre_archivo_original = os.path.basename(archivo_html_key).lower()","","    if \"eltiempo\" in nombre_archivo_original:","        periodico_identificado = \"eltiempo\"","        base_url = BASE_URLS[periodico_identificado]","        datos_noticias = extract_data_eltiempo(soup, base_url)","    elif \"publimetro\" in nombre_archivo_original:","        periodico_identificado = \"publimetro\"","        base_url = BASE_URLS[periodico_identificado]","        datos_noticias = extract_data_publimetro(soup, base_url)","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. No se puede determinar la fuente (eltiempo/publimetro).\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido\"}","","    if not datos_noticias:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","        ","    # Preparar nombre y ruta para el archivo CSV","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","    ","    # Nombre del archivo CSV basado en el nombre del HTML original","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","    ","    # Ruta temporal en Lambda para guardar el CSV antes de subirlo","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\" ","    ","    # Escribir los datos al archivo CSV","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';') # Usando ; como delimitador","            ","            writer.writeheader()","            writer.writerows(datos_noticias)","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","        ","    # Ruta de destino en S3 para el archivo CSV","    # Formato: TRUSTED-LAYER/periodico={periodico}/year={YYYY}/month={MM}/day={DD}/{nombre_original}.csv","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","    ","    # Subir el archivo CSV al bucket de destino","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO_TRUSTED, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","        ","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv\": f\"s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\"","    }"],"id":21},{"start":{"row":0,"column":0},"end":{"row":207,"column":5},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os # Para manipulación de rutas y nombres de archivo","","# Configura el cliente S3","s3_client = boto3.client(\"s3\")","","# IMPORTANTE: REEMPLAZA \"TU_BUCKET_DE_DESTINO_AQUI\" ","# con el nombre real de tu bucket S3 para la capa \"trusted\".","BUCKET_DESTINO_TRUSTED = \"TU_BUCKET_DE_DESTINO_AQUI\" ","","# URLs base para construir enlaces absolutos si es necesario","BASE_URLS = {","    \"eltiempo\": \"https://www.eltiempo.com\",","    \"publimetro\": \"https://www.publimetro.co\" # Verifica si es .co, .com u otro para Colombia","}","","def app(event, context):","    \"\"\"","    Función principal de AWS Lambda.","    Parsea archivos HTML de noticias (El Tiempo o Publimetro) desde S3 (capa \"raw\"),","    extrae categoría, titular y enlace, y guarda los resultados en un CSV ","    en S3 (capa \"trusted\") con la estructura:","    TRUSTED-LAYER/periodico={periodico}/year={YYYY}/month={MM}/day={DD}/{nombre_original}.csv","    \"\"\"","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"] ","    ","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","    ","    # Obtener el contenido del archivo HTML desde S3","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        # Si no se puede leer el archivo, no se puede continuar.","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","        ","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    noticias_extraidas = []","    periodico_identificado = None","    base_url_periodico = None","    ","    # Determinar el periódico basado en el nombre del archivo","    # Esto es importante para seleccionar los parsers correctos y la ruta de guardado","    nombre_archivo_original_lower = os.path.basename(archivo_html_key).lower()","","    # --- Lógica de extracción específica para cada periódico ---","    if \"eltiempo\" in nombre_archivo_original_lower:","        periodico_identificado = \"eltiempo\"","        base_url_periodico = BASE_URLS[periodico_identificado]","        ","        # Selectores para El Tiempo (basados en el HTML 'eltiempo-contenido-2025-05-29.html')","        # Contenedor principal de cada noticia en la lista","        article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","        print(f\"Identificado como El Tiempo. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\" # Valor por defecto si no se encuentra","            titular_texto = None","            enlace_url = None","            try:","                # Extracción de la categoría","                category_tag = item_html.find(\"div\", class_=\"category\")","                if category_tag:","                    categoria_texto = category_tag.get_text(strip=True)","","                # Extracción del titular y enlace","                title_container = item_html.find(\"h3\", class_=\"title-container\")","                if title_container:","                    title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                    if title_link_tag:","                        titular_texto = title_link_tag.get_text(strip=True)","                        href = title_link_tag.get('href')","                        if href:","                            # Asegurar que el enlace sea absoluto","                            if href.startswith(\"http\"):","                                enlace_url = href","                            else:","                                enlace_url = base_url_periodico + href.lstrip('/')","                ","                # Solo añadir si tenemos titular y enlace","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                # Loguear error y continuar con el siguiente item","                print(f\"Error parseando un item de El Tiempo: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\") ","                continue","","    elif \"publimetro\" in nombre_archivo_original_lower:","        periodico_identificado = \"publimetro\"","        base_url_periodico = BASE_URLS[periodico_identificado]","        ","        # Selectores para Publimetro (basados en el HTML 'publimetro-contenido-2025-05-29.html')","        # Contenedor principal de cada noticia","        article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","        print(f\"Identificado como Publimetro. Encontrados {len(article_items)} artículos potenciales.\")","        ","        for item_html in article_items:","            categoria_texto = \"N/A\" # Valor por defecto","            titular_texto = None","            enlace_url = None","            try:","                # Extracción del titular y enlace","                title_anchor = item_html.find(\"a\", class_=\"sm-promo-headline-link\")","                if title_anchor:","                    titular_texto = title_anchor.get_text(strip=True)","                    href = title_anchor.get('href')","                    if href:","                        # Asegurar que el enlace sea absoluto","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url_periodico + href.lstrip('/')","                ","                # Extracción de la categoría","                category_anchor = item_html.find(\"a\", class_=\"sm-promo-label-primary-link\")","                if category_anchor:","                    category_span = category_anchor.find(\"span\")","                    if category_span: # Preferir el texto dentro del span si existe","                        categoria_texto = category_span.get_text(strip=True)","                    else: # Sino, tomar el texto directo del enlace de categoría","                        categoria_texto = category_anchor.get_text(strip=True)","","                # Solo añadir si tenemos titular y enlace","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                # Loguear error y continuar con el siguiente item","                print(f\"Error parseando un item de Publimetro: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. El nombre no contiene 'eltiempo' ni 'publimetro'.\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido en el nombre del archivo.\"}","","    # --- Fin de la lógica de extracción ---","","    if not noticias_extraidas:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key} para {periodico_identificado}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","        ","    # Preparar nombre y ruta para el archivo CSV en S3","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","    ","    # Usar el nombre del archivo HTML original (sin extensión) para el CSV","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","    ","    # Ruta temporal en Lambda para guardar el CSV antes de subirlo","    # /tmp/ es el único directorio escribible en Lambda (aparte de /mnt/ si se monta EFS)","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\" ","    ","    # Escribir los datos al archivo CSV temporalmente","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            # Campos que se guardarán en el CSV","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","            ","            writer.writeheader() # Escribir la fila de encabezados","            writer.writerows(noticias_extraidas) # Escribir todas las noticias","        print(f\"Archivo CSV temporal creado en: {ruta_csv_temporal} con {len(noticias_extraidas)} filas de datos.\")","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","        ","    # Ruta de destino en S3 para el archivo CSV procesado","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","    ","    # Subir el archivo CSV al bucket de destino","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO_TRUSTED, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","    finally:","        # Limpiar el archivo temporal si existe","        if os.path.exists(ruta_csv_temporal):","            try:","                os.remove(ruta_csv_temporal)","                print(f\"Archivo temporal {ruta_csv_temporal} eliminado.\")","            except OSError as e:","                print(f\"Error eliminando el archivo temporal {ruta_csv_temporal}: {e}\")","","","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv_s3_path\": f\"s3://{BUCKET_DESTINO_TRUSTED}/{s3_key_destino_csv}\",","        \"records_processed\": len(noticias_extraidas)","    }"]}],[{"start":{"row":11,"column":26},"end":{"row":11,"column":51},"action":"remove","lines":["TU_BUCKET_DE_DESTINO_AQUI"],"id":22},{"start":{"row":11,"column":26},"end":{"row":11,"column":27},"action":"insert","lines":["r"]},{"start":{"row":11,"column":27},"end":{"row":11,"column":28},"action":"insert","lines":["e"]},{"start":{"row":11,"column":28},"end":{"row":11,"column":29},"action":"insert","lines":["c"]},{"start":{"row":11,"column":29},"end":{"row":11,"column":30},"action":"insert","lines":["i"]},{"start":{"row":11,"column":30},"end":{"row":11,"column":31},"action":"insert","lines":["b"]},{"start":{"row":11,"column":31},"end":{"row":11,"column":32},"action":"insert","lines":["e"]},{"start":{"row":11,"column":32},"end":{"row":11,"column":33},"action":"insert","lines":["i"]}],[{"start":{"row":11,"column":32},"end":{"row":11,"column":33},"action":"remove","lines":["i"],"id":23},{"start":{"row":11,"column":31},"end":{"row":11,"column":32},"action":"remove","lines":["e"]}],[{"start":{"row":11,"column":31},"end":{"row":11,"column":32},"action":"insert","lines":["e"],"id":24}],[{"start":{"row":11,"column":31},"end":{"row":11,"column":32},"action":"remove","lines":["e"],"id":25}],[{"start":{"row":11,"column":31},"end":{"row":11,"column":32},"action":"insert","lines":["i"],"id":26},{"start":{"row":11,"column":32},"end":{"row":11,"column":33},"action":"insert","lines":["e"]},{"start":{"row":11,"column":33},"end":{"row":11,"column":34},"action":"insert","lines":["n"]},{"start":{"row":11,"column":34},"end":{"row":11,"column":35},"action":"insert","lines":["d"]},{"start":{"row":11,"column":35},"end":{"row":11,"column":36},"action":"insert","lines":["o"]}],[{"start":{"row":9,"column":0},"end":{"row":10,"column":60},"action":"remove","lines":["# IMPORTANTE: REEMPLAZA \"TU_BUCKET_DE_DESTINO_AQUI\" ","# con el nombre real de tu bucket S3 para la capa \"trusted\"."],"id":27}],[{"start":{"row":5,"column":0},"end":{"row":6,"column":25},"action":"remove","lines":["","# Configura el cliente S3"],"id":28}],[{"start":{"row":11,"column":0},"end":{"row":11,"column":60},"action":"remove","lines":["# URLs base para construir enlaces absolutos si es necesario"],"id":29},{"start":{"row":10,"column":0},"end":{"row":11,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":13,"column":46},"end":{"row":13,"column":93},"action":"remove","lines":["# Verifica si es .co, .com u otro para Colombia"],"id":30}],[{"start":{"row":17,"column":0},"end":{"row":23,"column":7},"action":"remove","lines":["    \"\"\"","    Función principal de AWS Lambda.","    Parsea archivos HTML de noticias (El Tiempo o Publimetro) desde S3 (capa \"raw\"),","    extrae categoría, titular y enlace, y guarda los resultados en un CSV ","    en S3 (capa \"trusted\") con la estructura:","    TRUSTED-LAYER/periodico={periodico}/year={YYYY}/month={MM}/day={DD}/{nombre_original}.csv","    \"\"\""],"id":31}],[{"start":{"row":23,"column":0},"end":{"row":23,"column":52},"action":"remove","lines":["    # Obtener el contenido del archivo HTML desde S3"],"id":32},{"start":{"row":22,"column":4},"end":{"row":23,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":28,"column":0},"end":{"row":28,"column":64},"action":"remove","lines":["        # Si no se puede leer el archivo, no se puede continuar."],"id":33},{"start":{"row":27,"column":75},"end":{"row":28,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":35,"column":0},"end":{"row":37,"column":85},"action":"remove","lines":["    ","    # Determinar el periódico basado en el nombre del archivo","    # Esto es importante para seleccionar los parsers correctos y la ruta de guardado"],"id":34}],[{"start":{"row":38,"column":0},"end":{"row":38,"column":65},"action":"remove","lines":["    # --- Lógica de extracción específica para cada periódico ---"],"id":35}],[{"start":{"row":37,"column":0},"end":{"row":38,"column":0},"action":"remove","lines":["",""],"id":36}],[{"start":{"row":42,"column":0},"end":{"row":43,"column":58},"action":"remove","lines":["        # Selectores para El Tiempo (basados en el HTML 'eltiempo-contenido-2025-05-29.html')","        # Contenedor principal de cada noticia en la lista"],"id":37},{"start":{"row":41,"column":8},"end":{"row":42,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":46,"column":36},"end":{"row":46,"column":74},"action":"remove","lines":["# Valor por defecto si no se encuentra"],"id":38}],[{"start":{"row":50,"column":0},"end":{"row":50,"column":44},"action":"remove","lines":["                # Extracción de la categoría"],"id":39},{"start":{"row":49,"column":16},"end":{"row":50,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":54,"column":0},"end":{"row":54,"column":49},"action":"remove","lines":["                # Extracción del titular y enlace"],"id":40},{"start":{"row":53,"column":0},"end":{"row":54,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":67,"column":0},"end":{"row":67,"column":57},"action":"remove","lines":["                # Solo añadir si tenemos titular y enlace"],"id":41},{"start":{"row":66,"column":16},"end":{"row":67,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":61,"column":0},"end":{"row":61,"column":65},"action":"remove","lines":["                            # Asegurar que el enlace sea absoluto"],"id":42},{"start":{"row":60,"column":32},"end":{"row":61,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":73,"column":0},"end":{"row":73,"column":65},"action":"remove","lines":["                # Loguear error y continuar con el siguiente item"],"id":43},{"start":{"row":72,"column":34},"end":{"row":73,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":80,"column":0},"end":{"row":81,"column":46},"action":"remove","lines":["        # Selectores para Publimetro (basados en el HTML 'publimetro-contenido-2025-05-29.html')","        # Contenedor principal de cada noticia"],"id":44},{"start":{"row":79,"column":8},"end":{"row":80,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":88,"column":0},"end":{"row":88,"column":49},"action":"remove","lines":["                # Extracción del titular y enlace"],"id":45},{"start":{"row":87,"column":16},"end":{"row":88,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":99,"column":0},"end":{"row":99,"column":44},"action":"remove","lines":["                # Extracción de la categoría"],"id":46},{"start":{"row":98,"column":16},"end":{"row":99,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":107,"column":0},"end":{"row":107,"column":57},"action":"remove","lines":["                # Solo añadir si tenemos titular y enlace"],"id":47},{"start":{"row":106,"column":0},"end":{"row":107,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":114,"column":0},"end":{"row":114,"column":65},"action":"remove","lines":["                # Loguear error y continuar con el siguiente item"],"id":48},{"start":{"row":113,"column":34},"end":{"row":114,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":119,"column":0},"end":{"row":121,"column":0},"action":"remove","lines":["","    # --- Fin de la lógica de extracción ---",""],"id":49}],[{"start":{"row":124,"column":0},"end":{"row":124,"column":54},"action":"remove","lines":["    # Preparar nombre y ruta para el archivo CSV en S3"],"id":50},{"start":{"row":123,"column":8},"end":{"row":124,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":129,"column":0},"end":{"row":129,"column":74},"action":"remove","lines":["    # Usar el nombre del archivo HTML original (sin extensión) para el CSV"],"id":51},{"start":{"row":128,"column":4},"end":{"row":129,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":131,"column":0},"end":{"row":133,"column":89},"action":"remove","lines":["    ","    # Ruta temporal en Lambda para guardar el CSV antes de subirlo","    # /tmp/ es el único directorio escribible en Lambda (aparte de /mnt/ si se monta EFS)"],"id":52}],[{"start":{"row":133,"column":0},"end":{"row":134,"column":53},"action":"remove","lines":["    ","    # Escribir los datos al archivo CSV temporalmente"],"id":53}],[{"start":{"row":136,"column":0},"end":{"row":136,"column":47},"action":"remove","lines":["            # Campos que se guardarán en el CSV"],"id":54},{"start":{"row":135,"column":87},"end":{"row":136,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":9,"column":21},"end":{"row":9,"column":22},"action":"remove","lines":["D"],"id":55},{"start":{"row":9,"column":20},"end":{"row":9,"column":21},"action":"remove","lines":["E"]},{"start":{"row":9,"column":19},"end":{"row":9,"column":20},"action":"remove","lines":["T"]},{"start":{"row":9,"column":18},"end":{"row":9,"column":19},"action":"remove","lines":["S"]},{"start":{"row":9,"column":17},"end":{"row":9,"column":18},"action":"remove","lines":["U"]},{"start":{"row":9,"column":16},"end":{"row":9,"column":17},"action":"remove","lines":["R"]},{"start":{"row":9,"column":15},"end":{"row":9,"column":16},"action":"remove","lines":["T"]},{"start":{"row":9,"column":14},"end":{"row":9,"column":15},"action":"remove","lines":["_"]}],[{"start":{"row":10,"column":0},"end":{"row":14,"column":1},"action":"remove","lines":["","BASE_URLS = {","    \"eltiempo\": \"https://www.eltiempo.com\",","    \"publimetro\": \"https://www.publimetro.co\" ","}"],"id":56}],[{"start":{"row":36,"column":2},"end":{"row":36,"column":62},"action":"remove","lines":["      base_url_periodico = BASE_URLS[periodico_identificado]"],"id":57},{"start":{"row":36,"column":1},"end":{"row":36,"column":2},"action":"remove","lines":[" "]},{"start":{"row":36,"column":0},"end":{"row":36,"column":1},"action":"remove","lines":[" "]},{"start":{"row":35,"column":43},"end":{"row":36,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":73,"column":0},"end":{"row":73,"column":62},"action":"remove","lines":["        base_url_periodico = BASE_URLS[periodico_identificado]"],"id":58},{"start":{"row":72,"column":45},"end":{"row":73,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":145,"column":63},"end":{"row":145,"column":71},"action":"remove","lines":["_TRUSTED"],"id":59}],[{"start":{"row":146,"column":86},"end":{"row":146,"column":94},"action":"remove","lines":["_TRUSTED"],"id":60}],[{"start":{"row":164,"column":52},"end":{"row":164,"column":60},"action":"remove","lines":["_TRUSTED"],"id":61}],[{"start":{"row":0,"column":0},"end":{"row":166,"column":5},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os # Para manipulación de rutas y nombres de archivo","","s3_client = boto3.client(\"s3\")","","","BUCKET_DESTINO = \"recibiendo\" ","","","def app(event, context):","","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"] ","    ","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","    ","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","        ","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    noticias_extraidas = []","    periodico_identificado = None","    base_url_periodico = None","","    nombre_archivo_original_lower = os.path.basename(archivo_html_key).lower()","","    if \"eltiempo\" in nombre_archivo_original_lower:","        periodico_identificado = \"eltiempo\"","        ","        article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","        print(f\"Identificado como El Tiempo. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\" ","            titular_texto = None","            enlace_url = None","            try:","                category_tag = item_html.find(\"div\", class_=\"category\")","                if category_tag:","                    categoria_texto = category_tag.get_text(strip=True)","","                title_container = item_html.find(\"h3\", class_=\"title-container\")","                if title_container:","                    title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                    if title_link_tag:","                        titular_texto = title_link_tag.get_text(strip=True)","                        href = title_link_tag.get('href')","                        if href:","                            if href.startswith(\"http\"):","                                enlace_url = href","                            else:","                                enlace_url = base_url_periodico + href.lstrip('/')","                ","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de El Tiempo: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\") ","                continue","","    elif \"publimetro\" in nombre_archivo_original_lower:","        periodico_identificado = \"publimetro\"","        ","        article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","        print(f\"Identificado como Publimetro. Encontrados {len(article_items)} artículos potenciales.\")","        ","        for item_html in article_items:","            categoria_texto = \"N/A\" # Valor por defecto","            titular_texto = None","            enlace_url = None","            try:","                title_anchor = item_html.find(\"a\", class_=\"sm-promo-headline-link\")","                if title_anchor:","                    titular_texto = title_anchor.get_text(strip=True)","                    href = title_anchor.get('href')","                    if href:","                        # Asegurar que el enlace sea absoluto","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url_periodico + href.lstrip('/')","                ","                category_anchor = item_html.find(\"a\", class_=\"sm-promo-label-primary-link\")","                if category_anchor:","                    category_span = category_anchor.find(\"span\")","                    if category_span: # Preferir el texto dentro del span si existe","                        categoria_texto = category_span.get_text(strip=True)","                    else: # Sino, tomar el texto directo del enlace de categoría","                        categoria_texto = category_anchor.get_text(strip=True)","","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de Publimetro: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. El nombre no contiene 'eltiempo' ni 'publimetro'.\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido en el nombre del archivo.\"}","","    if not noticias_extraidas:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key} para {periodico_identificado}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","        ","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","    ","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\" ","","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","            ","            writer.writeheader() # Escribir la fila de encabezados","            writer.writerows(noticias_extraidas) # Escribir todas las noticias","        print(f\"Archivo CSV temporal creado en: {ruta_csv_temporal} con {len(noticias_extraidas)} filas de datos.\")","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","        ","    # Ruta de destino en S3 para el archivo CSV procesado","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","    ","    # Subir el archivo CSV al bucket de destino","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","    finally:","        # Limpiar el archivo temporal si existe","        if os.path.exists(ruta_csv_temporal):","            try:","                os.remove(ruta_csv_temporal)","                print(f\"Archivo temporal {ruta_csv_temporal} eliminado.\")","            except OSError as e:","                print(f\"Error eliminando el archivo temporal {ruta_csv_temporal}: {e}\")","","","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv_s3_path\": f\"s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\",","        \"records_processed\": len(noticias_extraidas)","    }"],"id":62},{"start":{"row":0,"column":0},"end":{"row":159,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os","","s3_client = boto3.client(\"s3\")","","BUCKET_DESTINO = \"recibiendo\"","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    noticias_extraidas = []","    periodico_identificado = None","    base_url_periodico = \"\"","","    nombre_archivo_original_lower = os.path.basename(archivo_html_key).lower()","","    if \"eltiempo\" in nombre_archivo_original_lower:","        periodico_identificado = \"eltiempo\"","        base_url_periodico = \"https://www.eltiempo.com\"","","        article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","        print(f\"Identificado como El Tiempo. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\"","            titular_texto = None","            enlace_url = None","            try:","                category_tag = item_html.find(\"div\", class_=\"category\")","                if category_tag:","                    categoria_texto = category_tag.get_text(strip=True)","","                title_container = item_html.find(\"h3\", class_=\"title-container\")","                if title_container:","                    title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                    if title_link_tag:","                        titular_texto = title_link_tag.get_text(strip=True)","                        href = title_link_tag.get('href')","                        if href:","                            if href.startswith(\"http\"):","                                enlace_url = href","                            else:","                                enlace_url = base_url_periodico + href.lstrip('/')","","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de El Tiempo: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","","    elif \"publimetro\" in nombre_archivo_original_lower:","        periodico_identificado = \"publimetro\"","        base_url_periodico = \"https://www.publimetro.co\"","","        article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","        print(f\"Identificado como Publimetro. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\"","            titular_texto = None","            enlace_url = None","            try:","                title_anchor = item_html.find(\"a\", class_=\"sm-promo-headline-link\")","                if title_anchor:","                    titular_texto = title_anchor.get_text(strip=True)","                    href = title_anchor.get('href')","                    if href:","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url_periodico + href.lstrip('/')","","                category_anchor = item_html.find(\"a\", class_=\"sm-promo-label-primary-link\")","                if category_anchor:","                    category_span = category_anchor.find(\"span\")","                    if category_span:","                        categoria_texto = category_span.get_text(strip=True)","                    else:","                        categoria_texto = category_anchor.get_text(strip=True)","","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de Publimetro: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. El nombre no contiene 'eltiempo' ni 'publimetro'.\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido en el nombre del archivo.\"}","","    if not noticias_extraidas:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key} para {periodico_identificado}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\"","","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","            writer.writeheader()","            writer.writerows(noticias_extraidas)","        print(f\"Archivo CSV temporal creado en: {ruta_csv_temporal} con {len(noticias_extraidas)} filas de datos.\")","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","    finally:","        if os.path.exists(ruta_csv_temporal):","            try:","                os.remove(ruta_csv_temporal)","                print(f\"Archivo temporal {ruta_csv_temporal} eliminado.\")","            except OSError as e:","                print(f\"Error eliminando el archivo temporal {ruta_csv_temporal}: {e}\")","","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv_s3_path\": f\"s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\",","        \"records_processed\": len(noticias_extraidas)","    }",""]}],[{"start":{"row":0,"column":0},"end":{"row":159,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","import os","","s3_client = boto3.client(\"s3\")","","BUCKET_DESTINO = \"recibiendo\"","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","","    print(f\"Procesando archivo: {archivo_html_key} desde el bucket: {bucket_origen}\")","","    try:","        response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html_key)","        contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    except Exception as e:","        print(f\"Error al obtener el archivo {archivo_html_key} de S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al obtener el archivo de S3: {e}\"}","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    noticias_extraidas = []","    periodico_identificado = None","    base_url_periodico = \"\"","","    nombre_archivo_original_lower = os.path.basename(archivo_html_key).lower()","","    if \"eltiempo\" in nombre_archivo_original_lower:","        periodico_identificado = \"eltiempo\"","        base_url_periodico = \"https://www.eltiempo.com\"","","        article_items = soup.find_all(\"div\", class_=\"row news-feed-list-element\")","        print(f\"Identificado como El Tiempo. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\"","            titular_texto = None","            enlace_url = None","            try:","                category_tag = item_html.find(\"div\", class_=\"category\")","                if category_tag:","                    categoria_texto = category_tag.get_text(strip=True)","","                title_container = item_html.find(\"h3\", class_=\"title-container\")","                if title_container:","                    title_link_tag = title_container.find(\"a\", class_=\"title page-link\")","                    if title_link_tag:","                        titular_texto = title_link_tag.get_text(strip=True)","                        href = title_link_tag.get('href')","                        if href:","                            if href.startswith(\"http\"):","                                enlace_url = href","                            else:","                                enlace_url = base_url_periodico + href.lstrip('/')","","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de El Tiempo: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","","    elif \"publimetro\" in nombre_archivo_original_lower:","        periodico_identificado = \"publimetro\"","        base_url_periodico = \"https://www.publimetro.co\"","","        article_items = soup.find_all(\"div\", class_=\"results-list--item-container\")","        print(f\"Identificado como Publimetro. Encontrados {len(article_items)} artículos potenciales.\")","","        for item_html in article_items:","            categoria_texto = \"N/A\"","            titular_texto = None","            enlace_url = None","            try:","                title_anchor = item_html.find(\"a\", class_=\"sm-promo-headline-link\")","                if title_anchor:","                    titular_texto = title_anchor.get_text(strip=True)","                    href = title_anchor.get('href')","                    if href:","                        if href.startswith(\"http\"):","                            enlace_url = href","                        else:","                            enlace_url = base_url_periodico + href.lstrip('/')","","                category_anchor = item_html.find(\"a\", class_=\"sm-promo-label-primary-link\")","                if category_anchor:","                    category_span = category_anchor.find(\"span\")","                    if category_span:","                        categoria_texto = category_span.get_text(strip=True)","                    else:","                        categoria_texto = category_anchor.get_text(strip=True)","","                if titular_texto and enlace_url:","                    noticias_extraidas.append({","                        \"categoria\": categoria_texto,","                        \"titular\": titular_texto,","                        \"enlace\": enlace_url","                    })","            except Exception as e:","                print(f\"Error parseando un item de Publimetro: {e}. Item HTML (primeros 200 chars): {str(item_html)[:200]}\")","                continue","    else:","        print(f\"Periódico no reconocido para el archivo: {archivo_html_key}. El nombre no contiene 'eltiempo' ni 'publimetro'.\")","        return {\"status\": \"error\", \"message\": \"Periódico no reconocido en el nombre del archivo.\"}","","    if not noticias_extraidas:","        print(f\"No se extrajeron noticias del archivo {archivo_html_key} para {periodico_identificado}. No se creará CSV.\")","        return {\"status\": \"success\", \"message\": \"No se extrajo información, CSV no creado.\"}","","    now = datetime.datetime.now()","    year = now.strftime(\"%Y\")","    month = now.strftime(\"%m\")","    day = now.strftime(\"%d\")","","    nombre_base_sin_extension = os.path.splitext(os.path.basename(archivo_html_key))[0]","    nombre_csv = f\"{nombre_base_sin_extension}.csv\"","    ruta_csv_temporal = f\"/tmp/{nombre_csv}\"","","    try:","        with open(ruta_csv_temporal, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","            campos = ['categoria', 'titular', 'enlace']","            writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","            writer.writeheader()","            writer.writerows(noticias_extraidas)","        print(f\"Archivo CSV temporal creado en: {ruta_csv_temporal} con {len(noticias_extraidas)} filas de datos.\")","    except IOError as e:","        print(f\"Error al escribir el archivo CSV en /tmp: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al escribir CSV temporal: {e}\"}","","    s3_key_destino_csv = f\"TRUSTED-LAYER/periodico={periodico_identificado}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    try:","        s3_client.upload_file(ruta_csv_temporal, BUCKET_DESTINO, s3_key_destino_csv)","        print(f\"Archivo CSV '{nombre_csv}' subido exitosamente a: s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\")","    except Exception as e:","        print(f\"Error al subir el archivo CSV '{nombre_csv}' a S3: {e}\")","        return {\"status\": \"error\", \"message\": f\"Error al subir CSV a S3: {e}\"}","    finally:","        if os.path.exists(ruta_csv_temporal):","            try:","                os.remove(ruta_csv_temporal)","                print(f\"Archivo temporal {ruta_csv_temporal} eliminado.\")","            except OSError as e:","                print(f\"Error eliminando el archivo temporal {ruta_csv_temporal}: {e}\")","","    return {","        \"status\": \"success\",","        \"message\": \"Procesamiento completado y CSV subido.\",","        \"source_file\": archivo_html_key,","        \"output_csv_s3_path\": f\"s3://{BUCKET_DESTINO}/{s3_key_destino_csv}\",","        \"records_processed\": len(noticias_extraidas)","    }",""],"id":63},{"start":{"row":0,"column":0},"end":{"row":49,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"casas-final-mitula\"  # Cambia si es un bucket distinto para noticias","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    noticias = []","","    for article in soup.find_all(\"article\"):","        titulo = article.find(\"h3\")","        descripcion = article.find(\"p\")","        fecha = article.find(\"time\")","","        noticia = {","            \"FechaDescarga\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),","            \"FechaPublicacion\": fecha.text.strip() if fecha else None,","            \"Titulo\": titulo.text.strip() if titulo else None,","            \"Descripcion\": descripcion.text.strip() if descripcion else None,","        }","","        noticias.append(noticia)","","    fecha_hoy = datetime.datetime.now().strftime(\"%Y-%m-%d\")","    name_file = archivo_html.replace(\".html\", \"\")","    nombre_csv = f\"{name_file}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['FechaDescarga', 'FechaPublicacion', 'Titulo', 'Descripcion']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, f\"{fecha_hoy}/{nombre_csv}\")","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{fecha_hoy}/{nombre_csv}\")","    ","    return {}",""]}],[{"start":{"row":0,"column":0},"end":{"row":49,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"casas-final-mitula\"  # Cambia si es un bucket distinto para noticias","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    ","    noticias = []","","    for article in soup.find_all(\"article\"):","        titulo = article.find(\"h3\")","        descripcion = article.find(\"p\")","        fecha = article.find(\"time\")","","        noticia = {","            \"FechaDescarga\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),","            \"FechaPublicacion\": fecha.text.strip() if fecha else None,","            \"Titulo\": titulo.text.strip() if titulo else None,","            \"Descripcion\": descripcion.text.strip() if descripcion else None,","        }","","        noticias.append(noticia)","","    fecha_hoy = datetime.datetime.now().strftime(\"%Y-%m-%d\")","    name_file = archivo_html.replace(\".html\", \"\")","    nombre_csv = f\"{name_file}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['FechaDescarga', 'FechaPublicacion', 'Titulo', 'Descripcion']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, f\"{fecha_hoy}/{nombre_csv}\")","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{fecha_hoy}/{nombre_csv}\")","    ","    return {}",""],"id":64},{"start":{"row":0,"column":0},"end":{"row":59,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"casas-final-mitula\"  # Cambia el nombre si usas otro bucket","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    noticias = []","","    for article in soup.find_all(\"article\"):","        categoria = article.find(\"span\", class_=\"category\") or article.find(\"a\", class_=\"category\")","        titulo = article.find(\"h3\") or article.find(\"h2\")","        enlace = article.find(\"a\", href=True)","","        noticia = {","            \"FechaDescarga\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),","            \"Categoria\": categoria.text.strip() if categoria else None,","            \"Titular\": titulo.text.strip() if titulo else None,","            \"Enlace\": enlace[\"href\"] if enlace else None,","        }","","        noticias.append(noticia)","","    ahora = datetime.datetime.now()","    year = ahora.strftime(\"%Y\")","    month = ahora.strftime(\"%m\")","    day = ahora.strftime(\"%d\")","    fecha_hoy = ahora.strftime(\"%Y-%m-%d\")","","    # Extraer nombre del periódico desde el nombre del archivo","    # Ej: noticias_eltiempo.html → periodico=eltiempo","    periodico = archivo_html.split(\"/\")[-1].replace(\".html\", \"\").replace(\"noticias_\", \"\")","","    nombre_csv = f\"{periodico}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['FechaDescarga', 'Categoria', 'Titular', 'Enlace']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    ","    return {}",""]}],[{"start":{"row":6,"column":18},"end":{"row":6,"column":36},"action":"remove","lines":["casas-final-mitula"],"id":65},{"start":{"row":6,"column":18},"end":{"row":6,"column":19},"action":"insert","lines":["r"]},{"start":{"row":6,"column":19},"end":{"row":6,"column":20},"action":"insert","lines":["e"]},{"start":{"row":6,"column":20},"end":{"row":6,"column":21},"action":"insert","lines":["c"]},{"start":{"row":6,"column":21},"end":{"row":6,"column":22},"action":"insert","lines":["i"]},{"start":{"row":6,"column":22},"end":{"row":6,"column":23},"action":"insert","lines":["b"]},{"start":{"row":6,"column":23},"end":{"row":6,"column":24},"action":"insert","lines":["i"]},{"start":{"row":6,"column":24},"end":{"row":6,"column":25},"action":"insert","lines":["e"]},{"start":{"row":6,"column":25},"end":{"row":6,"column":26},"action":"insert","lines":["n"]},{"start":{"row":6,"column":26},"end":{"row":6,"column":27},"action":"insert","lines":["d"]},{"start":{"row":6,"column":27},"end":{"row":6,"column":28},"action":"insert","lines":["o"]}],[{"start":{"row":0,"column":0},"end":{"row":59,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"recibiendo\"  # Cambia el nombre si usas otro bucket","","def app(event, context):","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","","    soup = BeautifulSoup(contenido_html, \"html.parser\")","","    noticias = []","","    for article in soup.find_all(\"article\"):","        categoria = article.find(\"span\", class_=\"category\") or article.find(\"a\", class_=\"category\")","        titulo = article.find(\"h3\") or article.find(\"h2\")","        enlace = article.find(\"a\", href=True)","","        noticia = {","            \"FechaDescarga\": datetime.datetime.now().strftime(\"%Y-%m-%d\"),","            \"Categoria\": categoria.text.strip() if categoria else None,","            \"Titular\": titulo.text.strip() if titulo else None,","            \"Enlace\": enlace[\"href\"] if enlace else None,","        }","","        noticias.append(noticia)","","    ahora = datetime.datetime.now()","    year = ahora.strftime(\"%Y\")","    month = ahora.strftime(\"%m\")","    day = ahora.strftime(\"%d\")","    fecha_hoy = ahora.strftime(\"%Y-%m-%d\")","","    # Extraer nombre del periódico desde el nombre del archivo","    # Ej: noticias_eltiempo.html → periodico=eltiempo","    periodico = archivo_html.split(\"/\")[-1].replace(\".html\", \"\").replace(\"noticias_\", \"\")","","    nombre_csv = f\"{periodico}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['FechaDescarga', 'Categoria', 'Titular', 'Enlace']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    ruta_s3 = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    ","    return {}",""],"id":70},{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"headlines\"","","def app(event, context):","    # Obtener información del evento","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","    ","    # Leer archivo desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    ","    # Detectar el periódico por el contenido","    if \"eltiempo.com\" in contenido_html:","        periodico = \"eltiempo\"","    elif \"publimetro.co\" in contenido_html:","        periodico = \"publimetro\"","    else:","        periodico = \"desconocido\"","    ","    # Parsear HTML","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    noticias = []","","    # El Tiempo","    if periodico == \"eltiempo\":","        for card in soup.select(\"article\"):","            categoria = card.find(\"span\", class_=\"category\") or card.find(\"span\", class_=\"kicker\")","            titular = card.find(\"h3\") or card.find(\"h2\")","            link = card.find(\"a\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"] if link[\"href\"].startswith(\"/\") else link[\"href\"]","                })","","    # Publimetro","    elif periodico == \"publimetro\":","        for card in soup.find_all(\"div\", class_=\"article-details\"):","            categoria = card.find(\"a\", class_=\"category\")","            titular = card.find(\"a\", class_=\"title\")","            link = card.find(\"a\", class_=\"title\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": link[\"href\"]","                })","","    # Obtener fecha actual","    fecha_hoy = datetime.datetime.now()","    year = fecha_hoy.strftime(\"%Y\")","    month = fecha_hoy.strftime(\"%m\")","    day = fecha_hoy.strftime(\"%d\")","    nombre_csv = f\"{periodico}_{year}-{month}-{day}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    # Guardar CSV localmente","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir a S3 en ruta organizada","    ruta_s3 = f\"final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    return {}",""]}],[{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"headlines\"","","def app(event, context):","    # Obtener información del evento","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","    ","    # Leer archivo desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    ","    # Detectar el periódico por el contenido","    if \"eltiempo.com\" in contenido_html:","        periodico = \"eltiempo\"","    elif \"publimetro.co\" in contenido_html:","        periodico = \"publimetro\"","    else:","        periodico = \"desconocido\"","    ","    # Parsear HTML","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    noticias = []","","    # El Tiempo","    if periodico == \"eltiempo\":","        for card in soup.select(\"article\"):","            categoria = card.find(\"span\", class_=\"category\") or card.find(\"span\", class_=\"kicker\")","            titular = card.find(\"h3\") or card.find(\"h2\")","            link = card.find(\"a\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"] if link[\"href\"].startswith(\"/\") else link[\"href\"]","                })","","    # Publimetro","    elif periodico == \"publimetro\":","        for card in soup.find_all(\"div\", class_=\"article-details\"):","            categoria = card.find(\"a\", class_=\"category\")","            titular = card.find(\"a\", class_=\"title\")","            link = card.find(\"a\", class_=\"title\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": link[\"href\"]","                })","","    # Obtener fecha actual","    fecha_hoy = datetime.datetime.now()","    year = fecha_hoy.strftime(\"%Y\")","    month = fecha_hoy.strftime(\"%m\")","    day = fecha_hoy.strftime(\"%d\")","    nombre_csv = f\"{periodico}_{year}-{month}-{day}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    # Guardar CSV localmente","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir a S3 en ruta organizada","    ruta_s3 = f\"final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    return {}",""],"id":71},{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"action":"insert","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"headlines\"","","def app(event, context):","    # Obtener información del evento","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","    ","    # Leer archivo desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    ","    # Detectar el periódico por el contenido","    if \"eltiempo.com\" in contenido_html:","        periodico = \"eltiempo\"","    elif \"publimetro.co\" in contenido_html:","        periodico = \"publimetro\"","    else:","        periodico = \"desconocido\"","    ","    # Parsear HTML","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    noticias = []","","    # El Tiempo","    if periodico == \"eltiempo\":","        for card in soup.select(\"article\"):","            categoria = card.find(\"span\", class_=\"category\") or card.find(\"span\", class_=\"kicker\")","            titular = card.find(\"h3\") or card.find(\"h2\")","            link = card.find(\"a\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"] if link[\"href\"].startswith(\"/\") else link[\"href\"]","                })","","    # Publimetro","    elif periodico == \"publimetro\":","        for card in soup.find_all(\"div\", class_=\"article-details\"):","            categoria = card.find(\"a\", class_=\"category\")","            titular = card.find(\"a\", class_=\"title\")","            link = card.find(\"a\", class_=\"title\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": link[\"href\"]","                })","","    # Obtener fecha actual","    fecha_hoy = datetime.datetime.now()","    year = fecha_hoy.strftime(\"%Y\")","    month = fecha_hoy.strftime(\"%m\")","    day = fecha_hoy.strftime(\"%d\")","    nombre_csv = f\"{periodico}_{year}-{month}-{day}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    # Guardar CSV localmente","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir a S3 en ruta organizada","    ruta_s3 = f\"final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    return {}",""]}],[{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"action":"remove","lines":["import boto3","import csv","import datetime","from bs4 import BeautifulSoup","","s3_client = boto3.client(\"s3\")","BUCKET_DESTINO = \"headlines\"","","def app(event, context):","    # Obtener información del evento","    bucket_origen = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]","    archivo_html = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]","    print(f\"Procesando archivo: {archivo_html} desde {bucket_origen}\")","    ","    # Leer archivo desde S3","    response = s3_client.get_object(Bucket=bucket_origen, Key=archivo_html)","    contenido_html = response[\"Body\"].read().decode(\"utf-8\")","    ","    # Detectar el periódico por el contenido","    if \"eltiempo.com\" in contenido_html:","        periodico = \"eltiempo\"","    elif \"publimetro.co\" in contenido_html:","        periodico = \"publimetro\"","    else:","        periodico = \"desconocido\"","    ","    # Parsear HTML","    soup = BeautifulSoup(contenido_html, \"html.parser\")","    noticias = []","","    # El Tiempo","    if periodico == \"eltiempo\":","        for card in soup.select(\"article\"):","            categoria = card.find(\"span\", class_=\"category\") or card.find(\"span\", class_=\"kicker\")","            titular = card.find(\"h3\") or card.find(\"h2\")","            link = card.find(\"a\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": \"https://www.eltiempo.com\" + link[\"href\"] if link[\"href\"].startswith(\"/\") else link[\"href\"]","                })","","    # Publimetro","    elif periodico == \"publimetro\":","        for card in soup.find_all(\"div\", class_=\"article-details\"):","            categoria = card.find(\"a\", class_=\"category\")","            titular = card.find(\"a\", class_=\"title\")","            link = card.find(\"a\", class_=\"title\")","","            if categoria and titular and link:","                noticias.append({","                    \"Categoria\": categoria.get_text(strip=True),","                    \"Titular\": titular.get_text(strip=True),","                    \"Link\": link[\"href\"]","                })","","    # Obtener fecha actual","    fecha_hoy = datetime.datetime.now()","    year = fecha_hoy.strftime(\"%Y\")","    month = fecha_hoy.strftime(\"%m\")","    day = fecha_hoy.strftime(\"%d\")","    nombre_csv = f\"{periodico}_{year}-{month}-{day}.csv\"","    ruta_csv = f\"/tmp/{nombre_csv}\"","","    # Guardar CSV localmente","    with open(ruta_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:","        campos = ['Categoria', 'Titular', 'Link']","        writer = csv.DictWriter(csvfile, fieldnames=campos, delimiter=';')","        writer.writeheader()","        writer.writerows(noticias)","","    # Subir a S3 en ruta organizada","    ruta_s3 = f\"final/periodico={periodico}/year={year}/month={month}/day={day}/{nombre_csv}\"","    s3_client.upload_file(ruta_csv, BUCKET_DESTINO, ruta_s3)","","    print(f\"Archivo CSV subido: s3://{BUCKET_DESTINO}/{ruta_s3}\")","    return {}",""],"id":72},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":17,"column":49},"end":{"row":17,"column":57},"action":"remove","lines":["parci4l3"],"id":73},{"start":{"row":17,"column":49},"end":{"row":17,"column":50},"action":"insert","lines":["r"]},{"start":{"row":17,"column":50},"end":{"row":17,"column":51},"action":"insert","lines":["e"]},{"start":{"row":17,"column":51},"end":{"row":17,"column":52},"action":"insert","lines":["c"]},{"start":{"row":17,"column":52},"end":{"row":17,"column":53},"action":"insert","lines":["i"]},{"start":{"row":17,"column":53},"end":{"row":17,"column":54},"action":"insert","lines":["b"]},{"start":{"row":17,"column":54},"end":{"row":17,"column":55},"action":"insert","lines":["i"]},{"start":{"row":17,"column":55},"end":{"row":17,"column":56},"action":"insert","lines":["e"]},{"start":{"row":17,"column":56},"end":{"row":17,"column":57},"action":"insert","lines":["n"]},{"start":{"row":17,"column":57},"end":{"row":17,"column":58},"action":"insert","lines":["d"]},{"start":{"row":17,"column":58},"end":{"row":17,"column":59},"action":"insert","lines":["o"]}],[{"start":{"row":12,"column":4},"end":{"row":12,"column":18},"action":"remove","lines":["lambda_handler"],"id":74},{"start":{"row":12,"column":4},"end":{"row":12,"column":5},"action":"insert","lines":["a"]},{"start":{"row":12,"column":5},"end":{"row":12,"column":6},"action":"insert","lines":["p"]},{"start":{"row":12,"column":6},"end":{"row":12,"column":7},"action":"insert","lines":["p"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def app(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'recibiendo')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":75},{"start":{"row":0,"column":0},"end":{"row":314,"column":5},"action":"insert","lines":["stray kid fan #8","nyarancias","En una llamada","","mateokb19"," añadió a ","stray kid fan #8"," al grupo. — 8:46 p. m.","mateokb19"," inició una llamada. — 8:46 p. m.","valuwu — 9:51 p. m.","import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","... (Quedan 51 líneas)","Minimizar","message.txt","6 KB","{","    \"dev2\": {","        \"app_function\": \"procesador.lambda_handler\",","        \"aws_region\": \"us-east-1\",","        \"environment_variables\": {","            \"BUCKET_NAME\": \"parci4l3\",","            \"PUBLIMETRO_URL\": \"https://www.publimetro.co/\",","            \"ELTIEMPO_URL\": \"https://www.eltiempo.com/\"","        },","        \"exclude\": [","            \"boto3\",","            \"dateutil\",","            \"botocore\",","            \"s3transfer\",","            \"concurrent\"","        ],","        \"project_name\": \"null\",","        \"runtime\": \"python3.10\",","        \"s3_bucket\": \"parci4l3\",","        \"keep_warm\": false,","        \"apigateway_enabled\": false,","        \"manage_roles\": false,","        \"role_name\": \"LabRole\",","        \"events\": [","            {","                \"function\": \"procesador.lambda_handler\",","                \"event_source\": {","                    \"arn\": \"arn:aws:s3:::parci4l3\",","                    \"events\": [\"s3:ObjectCreated:*\"],","                    \"filters\": {","                        \"Key\": {","                            \"FilterRules\": [","                                {","                                    \"Name\": \"prefix\",","                                    \"Value\": \"headlines/raw/\"","                                },","                                {","                                    \"Name\": \"suffix\",","                                    \"Value\": \".html\"","                                }","                            ]","                        }","                    }","                }","            }","        ]","    }","}","﻿","import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":162,"column":1},"action":"remove","lines":["stray kid fan #8","nyarancias","En una llamada","","mateokb19"," añadió a ","stray kid fan #8"," al grupo. — 8:46 p. m.","mateokb19"," inició una llamada. — 8:46 p. m.","valuwu — 9:51 p. m.","import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","... (Quedan 51 líneas)","Minimizar","message.txt","6 KB","{","    \"dev2\": {","        \"app_function\": \"procesador.lambda_handler\",","        \"aws_region\": \"us-east-1\",","        \"environment_variables\": {","            \"BUCKET_NAME\": \"parci4l3\",","            \"PUBLIMETRO_URL\": \"https://www.publimetro.co/\",","            \"ELTIEMPO_URL\": \"https://www.eltiempo.com/\"","        },","        \"exclude\": [","            \"boto3\",","            \"dateutil\",","            \"botocore\",","            \"s3transfer\",","            \"concurrent\"","        ],","        \"project_name\": \"null\",","        \"runtime\": \"python3.10\",","        \"s3_bucket\": \"parci4l3\",","        \"keep_warm\": false,","        \"apigateway_enabled\": false,","        \"manage_roles\": false,","        \"role_name\": \"LabRole\",","        \"events\": [","            {","                \"function\": \"procesador.lambda_handler\",","                \"event_source\": {","                    \"arn\": \"arn:aws:s3:::parci4l3\",","                    \"events\": [\"s3:ObjectCreated:*\"],","                    \"filters\": {","                        \"Key\": {","                            \"FilterRules\": [","                                {","                                    \"Name\": \"prefix\",","                                    \"Value\": \"headlines/raw/\"","                                },","                                {","                                    \"Name\": \"suffix\",","                                    \"Value\": \".html\"","                                }","                            ]","                        }","                    }","                }","            }","        ]","    }","}"],"id":76}],[{"start":{"row":1,"column":1},"end":{"row":2,"column":0},"action":"remove","lines":["",""],"id":77},{"start":{"row":1,"column":0},"end":{"row":1,"column":1},"action":"remove","lines":["﻿"]},{"start":{"row":0,"column":0},"end":{"row":1,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":78},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":17,"column":49},"end":{"row":17,"column":57},"action":"remove","lines":["parci4l3"],"id":79},{"start":{"row":17,"column":49},"end":{"row":17,"column":50},"action":"insert","lines":["r"]},{"start":{"row":17,"column":50},"end":{"row":17,"column":51},"action":"insert","lines":["e"]},{"start":{"row":17,"column":51},"end":{"row":17,"column":52},"action":"insert","lines":["c"]},{"start":{"row":17,"column":52},"end":{"row":17,"column":53},"action":"insert","lines":["i"]},{"start":{"row":17,"column":53},"end":{"row":17,"column":54},"action":"insert","lines":["b"]}],[{"start":{"row":17,"column":49},"end":{"row":17,"column":54},"action":"remove","lines":["recib"],"id":80},{"start":{"row":17,"column":49},"end":{"row":17,"column":59},"action":"insert","lines":["recibiendo"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'recibiendo')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":81},{"start":{"row":0,"column":0},"end":{"row":139,"column":0},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","s3 = boto3.client('s3')","","def app(event, context):  # <- Cambiado de lambda_handler a app","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'recibiendo')","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }",""]}],[{"start":{"row":11,"column":25},"end":{"row":11,"column":63},"action":"remove","lines":[" # <- Cambiado de lambda_handler a app"],"id":82}]]},"ace":{"folds":[],"scrolltop":1620,"scrollleft":0,"selection":{"start":{"row":11,"column":25},"end":{"row":11,"column":25},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":5,"state":"start","mode":"ace/mode/python"}},"timestamp":1748490525278}